# -*- coding: utf-8 -*-
"""custos_medicos_regressao_linear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I6epknr0s5aDUGIex8xikjeo4ZdzPusr

VARIÁVEL DE INTERESSE: CUSTO_SAUDE

ORDEM RECOMENDADA PARA VERIFICAÇÃO DOS PRESSUPOSTOS:



---




[1] Análise Exploratória e Tratamento de Outliers (IQR)
          ↓

---


[2] Verificação da Linearidade (Dispersão)
          ↓

---


[3] Verificação de Multicolinearidade (VIF)
          ↓

---


[4] Ajuste do Modelo de Regressão Linear
          ↓

---


[5] Teste de Autocorrelação (Durbin-Watson)
          ↓

---


[6] Teste de Homocedasticidade (Breusch-Pagan)
          ↓

---


[7] Teste de Normalidade dos Resíduos (Shapiro-Wilk, Q-Q plot)

#Análise dos Custos Médicos para aplicação de regressão linear.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.stattools import durbin_watson
from statsmodels.stats.diagnostic import het_breuschpagan

"""#Carregamento dos dados"""

custos_medicos = pd.read_csv('Custos_Medicos_Corrigido.csv')

custos_medicos

"""#Verificação de Outliers"""

for coluna in custos_medicos.columns:
    # Verifica se a coluna é numérica
    if custos_medicos[coluna].dtype in ['int64', 'float64']:
        # Calcula os quartis
        Q1 = np.percentile(custos_medicos[coluna], 25)  # Primeiro quartil (25%)
        Q3 = np.percentile(custos_medicos[coluna], 75)  # Terceiro quartil (75%)
        IQR = Q3 - Q1  # Intervalo interquartil

        # Calcula os limites inferior e superior
        LI = Q1 - 1.5 * IQR
        LS = Q3 + 1.5 * IQR

        # Identifica os outliers
        outliers = custos_medicos[(custos_medicos[coluna] < LI) | (custos_medicos[coluna] > LS)][coluna]

        # Exibe os outliers encontrados
        print(f"Outliers na coluna '{coluna}':")
        print(outliers)
        print(f"Total de outliers: {len(outliers)}\n")
    else:
        print(f"A coluna '{coluna}' não é numérica e será ignorada.\n")

sns.histplot(custos_medicos,x='Num_Cirurgias',kde=True)

sns.histplot(custos_medicos,x='Risco_Saude',kde=True)

"""##Risco_Saude: Como os dados não tem uma boa distribuição é necessário fazer um tratamento da coluna, para garantir que o modelo não vai ser enviesado por esses valores extremos.
##Porque não simplesmente remover os outliers? Pois parecem ser números reais de observações possíveis apesar de raras e pouco recorrentes. E dados com essas característica não podem ser removidas sem impactar negativamente o modelo.

##Num_Cirurgia: Como há apenas 1 outlier e esse outlier não afeta estatísticas importantes não será necessário fazer a transformação.

##Para Risco_Saude utlizaremos transformação logarítimica, ideal para valores positivos e um método muito eficaz para lidar com outliers. Ele tem a função de reduzir a amplitude dos valores e suaviza os outliers.
"""

custos_medicos['Risco_Saude'].describe()

custos_medicos['Risco_Saude'] = np.log(custos_medicos['Risco_Saude'])

sns.regplot(custos_medicos,x='Risco_Saude',y='Custo_Saude')

"""#Depois da tranformação os dados reduziu a correlação com a variável alvo."""

corr=custos_medicos.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr,cmap='coolwarm',annot=True)

"""#Agora farei o mesmo para o numero de cirurgias"""

custos_medicos['Num_Cirurgias'].describe()

corr=custos_medicos.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr,cmap='coolwarm',annot=True)

"""#Matriz de Correlação: Verificando a linearidade das variáveis explicativas com a variável alvo."""

corr=custos_medicos.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr,cmap='coolwarm',annot=True)

x=custos_medicos.drop('Custo_Saude',axis=1)
y=custos_medicos['Custo_Saude']

for coluna in custos_medicos.columns:
  if coluna != 'Custo_Saude':
    plt.figure(figsize=(8, 6))
    sns.regplot(x=coluna, y='Custo_Saude', data=custos_medicos,color='blue')
    plt.title(f'Dispersão {coluna} x Custo_Saude')
    plt.xlabel(coluna)
    plt.ylabel('Custo_Saude')
    plt.grid(True)
    plt.show()

"""# Verificando Multicolinearidade"""

# Criando a matriz de variáveis independentes (sem a variável dependente)
X = custos_medicos.drop(columns=["Custo_Saude"])

# Criando a variável dependente
X_const = sm.add_constant(X)  # Adicionando o intercepto

# Calculando o VIF para cada variável
vif_values = {X.columns[i]: variance_inflation_factor(X_const.values, i + 1) for i in range(len(X.columns))}

# Exibindo os resultados
print("Variância Inflation Factor (VIF) para cada variável:")
for var, vif in vif_values.items():
    status = "Possível multicolinearidade" if vif > 5 else "OK"
    print(f"{var}: VIF = {vif:.2f} → {status}")

"""#VERIFICANDO SE HÁ AUTOCORRELAÇÃO DOS RESÍDUOS"""

x_full=sm.add_constant(x)
modelo=sm.OLS(y,x_full).fit()
dw_stat=durbin_watson(modelo.resid)
if dw_stat <1.5:
  print('Forte autocorrelação positiva')
elif dw_stat > 2.5:
  print('Forte autocorrelação negativa')
else:
  print('Não há evidências fortes de autocorrelação nos resíduos')

def obter_residuos(x, y):
    modelo = sm.OLS(y, x).fit()
    return modelo.predict(x), modelo.resid

# Loop para verificar a influência de cada variável
for coluna in x.columns:
    x_reduzido = x.drop(columns=[coluna])  # Remove a variável atual
    x_reduzido_com_intercepto = sm.add_constant(x_reduzido)  # Adiciona intercepto

    # Obter resíduos
    _, residuos = obter_residuos(x_reduzido_com_intercepto, y)

    # Calcular Durbin-Watson
    dw_stat = durbin_watson(residuos)

    print(f"\nRemovendo {coluna}: Durbin-Watson = {dw_stat:.5f}")

"""#Ao retirar a variável Idade_Paciente o durbin watson fica bem próximo de dois e a autocorrelação dos resíduos some.

Verificando os resíduos para cada coluna
"""

def obter_residuos(x,y):
    modelo=LinearRegression()
    modelo.fit(x,y) # Changed x[coluna] to x[[coluna]] to fix the error ValueError: Expected 2D array, got 1D array instead:
    y_pred=modelo.predict(x) # Changed x[coluna] to x[[coluna]] to fix the error ValueError: Expected 2D array, got 1D array instead:
  #avoid NameError
    residuos = y - y_pred
    return y_pred, residuos

"""#VERIFICAÇÃO DE HOMOCEDASTICIDADE-Breusch-Pagan

"""

y_pred,residuo=obter_residuos(x,y)
sns.regplot(x=y_pred,y=residuo,color='red')
plt.title(f'Residuos x valores preditos')
plt.xlabel('Valores preditos')
plt.ylabel(f'Residuos ')
plt.show()

x_full=sm.add_constant(x)
y_pred,residuos=obter_residuos(x_full,y)
bp = het_breuschpagan(residuos, x_full)
print(f"Resultado: p-valor = {bp[1]:.5f}")

for coluna in x.columns:
    x_reduzido = x.drop(columns=[coluna])  # Remove a coluna

    # Adicionar o intercepto antes de treinar o modelo
    x_reduzido_com_intercepto = sm.add_constant(x_reduzido)

    # Obter resíduos
    y_pred, residuos = obter_residuos(x_reduzido_com_intercepto, y)

    # Teste de Breusch-Pagan para heterocedasticidade
    bp = het_breuschpagan(residuos, x_reduzido_com_intercepto)

    # Exibir resultado do teste para cada remoção de variável
    print(f"Resultado sem a coluna {coluna}: p-valor = {bp[1]:.5f}")

    # # Gráfico de resíduos
    # plt.scatter(y_pred, residuos)
    # plt.axhline(y=0, color='r', linestyle='--')
    # plt.xlabel("Valores Preditos")
    # plt.ylabel("Resíduos")
    # plt.title(f"Resíduos sem a variável {coluna}")
    # plt.show()

"""# Ao retirar as colunas:
* tempo_hospitalização
* risco_saude
* numeros_cirurgias
* gastos_ano_passado
##Os dados voltam a ser homocedasticos

Mas colunas como:

* gastos_ano_passado

##Não podem ser tiradas pois são muito importantes para o modelo
"""

from scipy.stats import shapiro

"""#Verificando a Normalidade"""

residuo.describe()

statistics,p_valor=shapiro(residuo)
if p_valor<0.05:
  print('Não é normal')
else:
  print('É normal')
print(p_valor)

# Create the histogram
plt.hist(residuo,
         # Remove kde=True
         # Add density=True to normalize the histogram to form a probability density
         density=True
        )
plt.xlabel('Resíduos')
plt.ylabel('Frequência')
plt.title('Histograma dos Resíduos')

# Overlay the KDE plot using seaborn
sns.kdeplot(residuo, color='blue', ax=plt.gca())  # Use gca() to get the current axes

plt.show()

"""#Verificando qual variável deve ser tirada para normalizar os resíduos."""

def testar_normalidade_removendo(custos_medicos, target):
    """
    Testa a normalidade dos resíduos removendo uma variável independente por vez.

    Parâmetros:
    custos_medicos (pd.DataFrame): O dataframe contendo os dados.
    target (str): A variável dependente do modelo.

    Retorna:
    dict: Contendo os p-valores do modelo completo e dos modelos reduzidos.
    """
    X = custos_medicos.drop(columns=[target])  # Variáveis preditoras
    y = custos_medicos[target]  # Variável dependente

    resultados = {}

    # Modelo completo
    X_const = sm.add_constant(X)
    modelo_completo = sm.OLS(y, X_const).fit()
    p_value_completo = shapiro(modelo_completo.resid)[1]

    # Testando a remoção de cada variável
    for var in X.columns:
        X_reduzido = X.drop(columns=[var])  # Removendo uma variável
        X_reduzido_const = sm.add_constant(X_reduzido)
        modelo_reduzido = sm.OLS(y, X_reduzido_const).fit()
        p_value_reduzido = shapiro(modelo_reduzido.resid)[1]

        resultados[var] = p_value_reduzido

    return p_value_completo, resultados

# Rodando o teste de normalidade retirando variáveis
p_valor_inicial, resultados_remocao = testar_normalidade_removendo(custos_medicos, "Custo_Saude")

# Exibindo os resultados
print(f"Teste de Shapiro-Wilk (Modelo Completo): p-valor = {p_valor_inicial:.5f}")

for var, p_valor in resultados_remocao.items():
    status = "Removendo essa variável resolve o problema" if p_valor > 0.05 else "Ainda há falta de normalidade"
    print(f"Removendo {var}: p-valor = {p_valor:.5f} → {status}")

"""# Ele pede para remover Num_Cirurgias;

#Modelo Final
"""

# Definindo a variável dependente e a variável independente escolhida
target = "Custo_Saude"
X_final = custos_medicos.drop(columns=['Num_Cirurgias','Idade_Paciente','Idade_Variavel','Risco_Saude','Custo_Saude','Num_Checkups','Tempo_Hospitalizacao'])  # Apenas a variável relevante
vif_values = {X_final.columns[i]: variance_inflation_factor(X_const.values, i + 1) for i in range(len(X_final.columns))}
print(vif_values)

y = custos_medicos[target]

# Adicionando o intercepto
X_final_const = sm.add_constant(X_final)

# Ajustando o modelo final
modelo_final = sm.OLS(y, X_final_const).fit()

bp_test = het_breuschpagan(modelo_final.resid, X_final_const)
p_value_bp = bp_test[1]
print(f"Teste de Breusch-Pagan: p-valor = {p_value_bp:.5f}")

p_value_completo = shapiro(modelo_final.resid)[1]
print(f"Shapiro: p-valor = {p_value_completo:.5f}")

# Exibindo o R² ajustado do modelo final
r2_ajustado_final = modelo_final.rsquared_adj
print(f"R² Ajustado do modelo final com apenas {X_final.columns}: {r2_ajustado_final:.5f}")

# Exibindo o resumo completo do modelo
print(modelo_final.summary())

"""#NUM_MEDICACÕES,NUM_CONSULTAS E HISTÓRICO FAMILIAR ESTÃO ATRAPALHANDO O MODELO.

#Ao retirar essas variáveis:
"""

# Definindo a variável dependente e a variável independente escolhida
target = "Custo_Saude"
X_final = custos_medicos.drop(columns=['Num_Cirurgias','Idade_Paciente','Idade_Variavel','Risco_Saude','Custo_Saude','Num_Checkups','Tempo_Hospitalizacao','Num_Medicacoes','Historico_Familiar'])  # Apenas a variável relevante
vif_values = {X_final.columns[i]: variance_inflation_factor(X_const.values, i + 1) for i in range(len(X_final.columns))}
print(vif_values)

y = custos_medicos[target]

# Adicionando o intercepto
X_final_const = sm.add_constant(X_final)

# Ajustando o modelo final
modelo_final = sm.OLS(y, X_final_const).fit()

bp_test = het_breuschpagan(modelo_final.resid, X_final_const)
p_value_bp = bp_test[1]
print(f"Teste de Breusch-Pagan: p-valor = {p_value_bp:.5f}")

p_value_completo = shapiro(modelo_final.resid)[1]
print(f"Shapiro: p-valor = {p_value_completo:.5f}")

# Exibindo o R² ajustado do modelo final
r2_ajustado_final = modelo_final.rsquared_adj
print(f"R² Ajustado do modelo final com apenas {X_final.columns}: {r2_ajustado_final:.5f}")

# Exibindo o resumo completo do modelo
print(modelo_final.summary())

"""#A NOTA [2] indica um problema que pode ser multicolinearidade. Como multicolinearidade já foi tratada, outra possibilidade são as diferenças de escalas entre as variáveis

#Normalização das variáveis explicativas
"""

def normalizar(x):
  return (x-np.min(x))/(np.max(x)-np.min(x))

def padronizar(x):
  return (x-np.mean(x))/np.std(x)

for coluna in X_final.columns:
  if coluna != 'Custo_Saude':
    statistics,p_valor=shapiro(X_final[coluna])
    print(f'O valor p da coluna {coluna}: ', p_valor)
    if p_valor<0.05:
      X_final[coluna]=normalizar(X_final[coluna])
    else:
      X_final[coluna]=padronizar(X_final[coluna])

X_final

"""#Resultado Final"""

y = custos_medicos[target]

# Adicionando o intercepto
X_final_const = sm.add_constant(X_final)

# Ajustando o modelo final
modelo_final = sm.OLS(y, X_final_const).fit()

bp_test = het_breuschpagan(modelo_final.resid, X_final_const)
p_value_bp = bp_test[1]
print(f"Teste de Breusch-Pagan: p-valor = {p_value_bp:.5f}")

p_value_completo = shapiro(modelo_final.resid)[1]
print(f"Shapiro: p-valor = {p_value_completo:.5f}")

# Exibindo o R² ajustado do modelo final
r2_ajustado_final = modelo_final.rsquared_adj
print(f"R² Ajustado do modelo final com apenas {X_final.columns}: {r2_ajustado_final:.5f}")

# Exibindo o resumo completo do modelo
print(modelo_final.summary())

"""#Observação: O R2 aparece como 1 devido a variável explicativa: GASTOS_ANO_PASSADO ter uma alta linearidade de aproximadamente 1. Dessa forma é natural os acertos do modelo terem valor 1.

##Conclusão: As variáveis ideais para manter um modelo robusto e garantindo que as 5 regras da regressão linear sejam cumpridas, são: Num_Consultas, Nivel_Atividade_Fisica, IMC e Gastos_Ano_Passado.
"""

